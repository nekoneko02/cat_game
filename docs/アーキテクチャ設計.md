# アーキテクチャ設計
この文書では、アーキテクチャを記載する。アーキテクチャの検討は以下を対象とする：
- 強化学習のモデル設計
- システム構成図
- リスク分析の結果、リスクが高のもの

## 強化学習のモデル設計
強化学習でねこのアクションやなつき度を学習する方針を定義する

### 基本概念
- なつき度 = ユーザーの近くにいることでの報酬
  - →近づく行動は、これで説明がつくが、その他の行動の定義が必要
- 遊ぶなど、個別ロジックは別のAIとして学習させる（？）
  - →Q値の合計を取る、拡張性も上がる

### コンポーネント
- なつき度
  - 飼い主=報酬を得られるもの　を学習する
  - 過ごす時間が長いほど、ポジティブな行動を取りやすくなる
- 個別アクション
  - 遊ぶ、愛でるなど、個別アクションの際に、とる行動を学習する
  - なつきには依存しない
- 基本アクション
  - 基本的なアクションの取りやすさを定義する
  - 疲れている=>寝やすい。空腹=>食べる。など
  - 強化学習ではなく、ロジックで記述する（予定）

上記の重み付き和とする

### なつき度
- 入力
  - ねこの位置
  - ターゲットの位置
- 出力
  - ポジ、ネガ、ニュートラルに対する確率（将来は拡張するが、いったん3次元）
- 報酬
  - ユーザーの基礎点
  - 個別アクションでの報酬
    - 例）楽しく遊ぶ
    - 例）おいしいおやつを食べる
    - 例）急に近づいてくる（負の報酬）
  - ポジティブな行動をとると、報酬がn倍
  - ニュートラルな行動をとると、報酬が1倍（そのまま）
  - ネガティブな行動をとると、報酬が-1倍
    - 例）危ないもの（負の報酬）から逃げる（ネガティブな行動）と正解
  - 飼い主は、一緒に過ごすほど正の報酬になる
    - 最初は怖い存在（負の報酬）
    - 徐々に（ねことって）良い存在（正の報酬）に変化する
  - 飼い主が正の報酬である場合、ゲームを続けてもらうこと=高い報酬となる
    - 特定のアクションの時にやめる＝その行動を好んでとる　という学習を期待する
    - 

### 個別アクション